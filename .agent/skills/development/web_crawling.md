# ì›¹ í¬ë¡¤ë§ ê°€ì´ë“œ: íŒêµë„ì„œê´€ ì²­êµ¬ê¸°í˜¸ ìˆ˜ì§‘

## ğŸ“‹ ê°œìš”

íŒêµë„ì„œê´€(ì„±ë‚¨ì‹œë¦½ë„ì„œê´€) ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë„ì„œ ì²­êµ¬ê¸°í˜¸ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ëŠ” ì›¹ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ì…ë‹ˆë‹¤.

---

## ğŸ¯ ì‚¬ìš© ì‚¬ë¡€

### ì–¸ì œ ì‚¬ìš©í•˜ë‚˜?
- ìƒˆë¡œìš´ ë„ì„œ ëª©ë¡(CSV/Excel)ì„ ë°›ì•˜ì„ ë•Œ
- ë°ì´í„°ë² ì´ìŠ¤ì— ì²­êµ¬ê¸°í˜¸ê°€ ì—†ëŠ” ì±…ë“¤ì´ ìˆì„ ë•Œ
- ê²¨ìš¸ë°©í•™/ì—¬ë¦„ë°©í•™ ë“± ì‹œì¦Œë³„ ì¶”ì²œ ë„ì„œ ì¶”ê°€ ì‹œ

### ê¸°ëŒ€ íš¨ê³¼
- ìˆ˜ë™ ê²€ìƒ‰ ëŒ€ë¹„ **ì‹œê°„ ì ˆì•½** (40ê¶Œ ê¸°ì¤€ ì•½ 1ì‹œê°„ â†’ 2ë¶„)
- **ì •í™•í•œ ì²­êµ¬ê¸°í˜¸** ìë™ ìˆ˜ì§‘
- **ì¼ê´„ ì—…ë°ì´íŠ¸** SQL ìë™ ìƒì„±

---

## ğŸ—ï¸ ì‹œìŠ¤í…œ êµ¬ì¡°

### íŒêµë„ì„œê´€ ì›¹ì‚¬ì´íŠ¸ ë¶„ì„

**ê³µì‹ URL**: https://www.snlib.go.kr/pg/index.do

**ê²€ìƒ‰ API ì—”ë“œí¬ì¸íŠ¸**:
```
https://www.snlib.go.kr/pg/plusSearchResultList.do
```

**ê²€ìƒ‰ íŒŒë¼ë¯¸í„°**:
```python
params = {
    'searchKeyword': 'ì±… ì œëª©',
    'searchType': 'SIMPLE',
    'searchCategory': 'BOOK',
    'searchLibraryArr': 'MP',  # íŒêµë„ì„œê´€ ì½”ë“œ
    'searchKey': 'ALL',
    'topSearchType': 'BOOK'
}
```

**HTML êµ¬ì¡°**:
```html
<ul class="resultList">
  <li>
    <dl>
      <dt><a>ì±… ì œëª©</a></dt>
      <dd>ì €ì : ...</dd>
      <dd>ë°œí–‰ì: ...</dd>
      <dd>ì²­êµ¬ê¸°í˜¸: ìœ  813.8-ã„±985ã„´</dd>  <!-- ì—¬ê¸°! -->
    </dl>
  </li>
</ul>
```

**CSS ì…€ë ‰í„°**:
- ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸: `ul.resultList > li`
- ì²­êµ¬ê¸°í˜¸: `dd` ìš”ì†Œ ì¤‘ "ì²­êµ¬ê¸°í˜¸:" í…ìŠ¤íŠ¸ í¬í•¨

---

## ğŸ”§ êµ¬í˜„ ë°©ë²•

### 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬

```python
import requests
from bs4 import BeautifulSoup
import json
import time
import re
```

### 2. í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸ í…œí”Œë¦¿

```python
import requests
from bs4 import BeautifulSoup
import time

BASE_URL = "https://www.snlib.go.kr/pg/plusSearchResultList.do"

def crawl_callno(title, author=''):
    """íŒêµë„ì„œê´€ì—ì„œ ì²­êµ¬ê¸°í˜¸ ê²€ìƒ‰"""
    
    params = {
        'searchKeyword': title,
        'searchType': 'SIMPLE',
        'searchCategory': 'BOOK',
        'searchLibraryArr': 'MP',  # íŒêµë„ì„œê´€
        'searchKey': 'ALL',
        'topSearchType': 'BOOK'
    }
    
    try:
        response = requests.get(BASE_URL, params=params, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ì²« ë²ˆì§¸ ê²€ìƒ‰ ê²°ê³¼
        result_list = soup.select('ul.resultList > li')
        
        if result_list:
            first_result = result_list[0]
            
            # ì²­êµ¬ê¸°í˜¸ ì°¾ê¸°
            dd_elements = first_result.select('dl dd')
            for dd in dd_elements:
                text = dd.get_text(strip=True)
                if 'ì²­êµ¬ê¸°í˜¸:' in text:
                    callno = text.replace('ì²­êµ¬ê¸°í˜¸:', '').strip()
                    # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
                    callno = callno.split('ìœ„ì¹˜ì¶œë ¥')[0].strip()
                    return callno
        
        return None
        
    except Exception as e:
        print(f"ì—ëŸ¬: {e}")
        return None

# ì‚¬ìš© ì˜ˆì‹œ
callno = crawl_callno("ë‚˜ëŠ” ì˜¤ëŠ˜ë„ ê°ì •ì‹ë‹¹ì— ê°€ìš”")
print(f"ì²­êµ¬ê¸°í˜¸: {callno}")
```

### 3. ì²­êµ¬ê¸°í˜¸ ë°ì´í„° ì •ì œ

í¬ë¡¤ë§í•œ ì²­êµ¬ê¸°í˜¸ì—ëŠ” ë¶ˆí•„ìš”í•œ ë©”íƒ€ë°ì´í„°ê°€ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
def clean_callno(raw_callno):
    """ì²­êµ¬ê¸°í˜¸ ì •ì œ"""
    if not raw_callno:
        return None
    
    # "ì €ì : ... ë°œí–‰ì: ... ë°œí–‰ì—°ë„: YYYY ì²­êµ¬ê¸°í˜¸" íŒ¨í„´
    parts = raw_callno.split('ë°œí–‰ì—°ë„:')
    if len(parts) >= 2:
        after_year = parts[-1].strip()
        # ì—°ë„ ì œê±° (4ìë¦¬ ìˆ«ì)
        callno = re.sub(r'^\d{4}\s+', '', after_year)
        return callno.strip()
    
    return raw_callno.strip()
```

**ì˜ˆì‹œ**:
```
ì…ë ¥: "ì €ì : ê¹€í˜„íƒœ ê¸€ ; ì˜¤ìˆ™ì§„ ê·¸ë¦¼ë°œí–‰ì: ê·¸ë¦°ë¶ë°œí–‰ì—°ë„: 2025 ìœ  813.8-ã„¹434-11"
ì¶œë ¥: "ìœ  813.8-ã„¹434-11"
```

---

## ğŸ“ ì „ì²´ ì›Œí¬í”Œë¡œìš°

### Step 1: ë„ì„œ ëª©ë¡ ì¤€ë¹„

CSV íŒŒì¼ í˜•ì‹:
```csv
title,author,publisher
ë‚˜ëŠ” ì˜¤ëŠ˜ë„ ê°ì •ì‹ë‹¹ì— ê°€ìš”,ê¹€í˜„íƒœ,ê·¸ë¦°ë¶
ì°½ë•ê¶ì— ë¶ˆì´ êº¼ì§€ë©´,ìµœì •í˜œ,ì±…ì½ëŠ”ê³°
```

### Step 2: í¬ë¡¤ë§ ì‹¤í–‰

```python
import json
import csv
import time

# CSV ì½ê¸°
books = []
with open('books.csv', 'r', encoding='utf-8') as f:
    reader = csv.DictReader(f)
    books = list(reader)

# í¬ë¡¤ë§
results = []
for i, book in enumerate(books, 1):
    print(f"[{i}/{len(books)}] {book['title']}")
    
    callno = crawl_callno(book['title'], book['author'])
    
    if callno:
        callno = clean_callno(callno)
        results.append({
            'title': book['title'],
            'callno': callno,
            'status': 'success'
        })
        print(f"  âœ… {callno}")
    else:
        results.append({
            'title': book['title'],
            'callno': None,
            'status': 'not_found'
        })
        print(f"  âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
    
    # ì„œë²„ ë¶€í•˜ ë°©ì§€
    time.sleep(1.5)

# ê²°ê³¼ ì €ì¥
with open('callno_results.json', 'w', encoding='utf-8') as f:
    json.dump(results, f, ensure_ascii=False, indent=2)
```

### Step 3: SQL ìƒì„±

```python
# UPDATE SQL ìƒì„±
sql_lines = []
sql_lines.append("-- ì²­êµ¬ê¸°í˜¸ ì—…ë°ì´íŠ¸")
sql_lines.append("")

for result in results:
    if result['status'] == 'success' and result['callno']:
        title_escaped = result['title'].replace("'", "''")
        callno_escaped = result['callno'].replace("'", "''")
        
        sql = f"""UPDATE childbook_items 
SET pangyo_callno = '{callno_escaped}'
WHERE title = '{title_escaped}';
"""
        sql_lines.append(f"-- {result['title']}")
        sql_lines.append(sql)

with open('update_callno.sql', 'w', encoding='utf-8') as f:
    f.write('\n'.join(sql_lines))
```

### Step 4: ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸

Supabase SQL Editorì—ì„œ `update_callno.sql` ì‹¤í–‰

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

### 1. ì„œë²„ ë¶€í•˜ ë°©ì§€

**í•„ìˆ˜**: ìš”ì²­ ê°„ 1-2ì´ˆ ëŒ€ê¸°
```python
time.sleep(1.5)  # 1.5ì´ˆ ëŒ€ê¸°
```

### 2. ì‹ ê°„ ë„ì„œ ë¯¸ë“±ë¡

**ë¬¸ì œ**: 2025ë…„ ì‹ ê°„ì€ ë„ì„œê´€ì— ì•„ì§ ë“±ë¡ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŒ

**í•´ê²°**:
- ì‹¤íŒ¨í•œ ì±… ëª©ë¡ì„ CSVë¡œ ì €ì¥
- 1-2ì£¼ í›„ ì¬í¬ë¡¤ë§
- ë˜ëŠ” ìˆ˜ë™ìœ¼ë¡œ ì²­êµ¬ê¸°í˜¸ í™•ì¸

### 3. ê²€ìƒ‰ ê²°ê³¼ ì •í™•ë„

**ë¬¸ì œ**: ë™ëª…ì´ì„œ(ê°™ì€ ì œëª©ì˜ ë‹¤ë¥¸ ì±…)ê°€ ìˆì„ ìˆ˜ ìˆìŒ

**í•´ê²°**:
- ì €ìëª…ìœ¼ë¡œ 2ì°¨ í•„í„°ë§
- ì¶œíŒì‚¬ ì •ë³´ë¡œ 3ì°¨ ê²€ì¦
- ê²°ê³¼ë¥¼ ìˆ˜ë™ìœ¼ë¡œ í™•ì¸

```python
# ì €ìëª…ìœ¼ë¡œ í•„í„°ë§ ì˜ˆì‹œ
author_name = author.split()[0]  # ì²« ë²ˆì§¸ ì €ìëª…
if author_name and author_name in result_author:
    # ë§¤ì¹­ ì„±ê³µ
    pass
```

---

## ğŸ“Š ì„±ê³µë¥  í–¥ìƒ íŒ

### 1. ì œëª© ì •ê·œí™”

```python
def normalize_title(title):
    """ì œëª© ì •ê·œí™”"""
    # ê´„í˜¸ ì œê±°
    title = re.sub(r'\([^)]*\)', '', title)
    # ë¶€ì œ ì œê±°
    title = title.split(':')[0]
    return title.strip()
```

### 2. ì¬ì‹œë„ ë¡œì§

```python
def crawl_with_retry(title, max_retries=3):
    """ì¬ì‹œë„ ë¡œì§"""
    for attempt in range(max_retries):
        try:
            result = crawl_callno(title)
            if result:
                return result
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
            else:
                raise
    return None
```

### 3. ìºì‹±

```python
import pickle

# ê²°ê³¼ ìºì‹±
cache = {}
try:
    with open('callno_cache.pkl', 'rb') as f:
        cache = pickle.load(f)
except:
    pass

def crawl_with_cache(title):
    if title in cache:
        return cache[title]
    
    result = crawl_callno(title)
    cache[title] = result
    
    with open('callno_cache.pkl', 'wb') as f:
        pickle.dump(cache, f)
    
    return result
```

---

## ğŸ“ ì‹¤ì „ ì˜ˆì‹œ: ê²¨ìš¸ë°©í•™ ë„ì„œ 40ê¶Œ

### ê²°ê³¼ ìš”ì•½
- **ì´**: 40ê¶Œ
- **ì„±ê³µ**: 22ê¶Œ (55%)
- **ì‹¤íŒ¨**: 18ê¶Œ (45% - ì‹ ê°„ ë¯¸ë“±ë¡)

### ìƒì„±ëœ íŒŒì¼
1. `winter_books_callno_results.json` - í¬ë¡¤ë§ ê²°ê³¼
2. `update_winter_callno_clean.sql` - UPDATE SQL (22ê¶Œ)
3. `winter_books_missing_callno.csv` - ì‹¤íŒ¨ ëª©ë¡ (18ê¶Œ)

### ì‹¤í–‰ ì‹œê°„
- í¬ë¡¤ë§: ì•½ 2ë¶„ (40ê¶Œ Ã— 1.5ì´ˆ + ë„¤íŠ¸ì›Œí¬ ì‹œê°„)
- SQL ìƒì„±: 1ì´ˆ ë¯¸ë§Œ

---

## ğŸ” íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: ì—°ê²° íƒ€ì„ì•„ì›ƒ

**ì¦ìƒ**: `requests.exceptions.Timeout`

**í•´ê²°**:
```python
response = requests.get(BASE_URL, params=params, timeout=15)  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
```

### ë¬¸ì œ 2: ì¸ì½”ë”© ì—ëŸ¬

**ì¦ìƒ**: í•œê¸€ì´ ê¹¨ì§

**í•´ê²°**:
```python
response.encoding = 'utf-8'
soup = BeautifulSoup(response.text, 'html.parser')
```

### ë¬¸ì œ 3: ì¤‘ë³µ ë°ì´í„°

**ì¦ìƒ**: ê°™ì€ ì±…ì´ ì—¬ëŸ¬ ë²ˆ INSERTë¨

**í•´ê²°**:
```sql
-- ì¤‘ë³µ ì œê±°
DELETE FROM childbook_items
WHERE id IN (
  SELECT id FROM (
    SELECT id, ROW_NUMBER() OVER (PARTITION BY title ORDER BY id) as rn
    FROM childbook_items
    WHERE curation_tag = 'ê²¨ìš¸ë°©í•™2026'
  ) t
  WHERE rn > 1
);
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ê´€ë ¨ íŒŒì¼
- `backend/crawl_pangyo_callno_v2.py` - í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸
- `backend/clean_callno_data.py` - ë°ì´í„° ì •ì œ ìŠ¤í¬ë¦½íŠ¸
- `backend/export_missing_books.py` - ì‹¤íŒ¨ ëª©ë¡ CSV ìƒì„±

### ì™¸ë¶€ ë¬¸ì„œ
- [BeautifulSoup ê³µì‹ ë¬¸ì„œ](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Requests ë¼ì´ë¸ŒëŸ¬ë¦¬](https://requests.readthedocs.io/)
- [ì„±ë‚¨ì‹œë¦½ë„ì„œê´€](https://www.snlib.go.kr/)

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

í¬ë¡¤ë§ ì‘ì—… ì‹œ í™•ì¸ ì‚¬í•­:

- [ ] ë„ì„œ ëª©ë¡ CSV/JSON íŒŒì¼ ì¤€ë¹„
- [ ] í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ (ì„œë²„ ë¶€í•˜ ì£¼ì˜)
- [ ] ê²°ê³¼ JSON íŒŒì¼ í™•ì¸
- [ ] ì²­êµ¬ê¸°í˜¸ ë°ì´í„° ì •ì œ
- [ ] UPDATE SQL ìƒì„±
- [ ] ì‹¤íŒ¨ ëª©ë¡ CSV ìƒì„± (ì¬ì‹œë„ìš©)
- [ ] Supabaseì—ì„œ SQL ì‹¤í–‰
- [ ] ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸ ì¿¼ë¦¬ ì‹¤í–‰
- [ ] ì¤‘ë³µ ë°ì´í„° í™•ì¸ ë° ì œê±°

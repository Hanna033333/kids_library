# -*- coding: utf-8 -*-
import sys
import io
import os
import json
import time
import requests
import re
from bs4 import BeautifulSoup

# ì½˜ì†” ì¶œë ¥ ì¸ì½”ë”© ì„¤ì •
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# Helper for logging
log_file = open('caldecott_crawling.log', 'w', encoding='utf-8')
def log(msg):
    print(msg)
    log_file.write(msg + '\n')
    log_file.flush()

# ì •ê·œí™” í•¨ìˆ˜: ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì œê±°
def normalize(text):
    if not text:
        return ""
    # ê´„í˜¸ì™€ ê·¸ ì•ˆì˜ ë‚´ìš© ì œê±°
    text = re.sub(r'\(.*?\)|\[.*?\]', '', text)
    # íŠ¹ìˆ˜ë¬¸ì ì œê±°, ê³µë°± ì œê±°, ì†Œë¬¸ì ë³€í™˜
    text = re.sub(r'[\s\W_]+', '', text).lower()
    return text

# JSON íŒŒì¼ ì½ê¸°
with open('caldecott_enriched.json', 'r', encoding='utf-8') as f:
    books = json.load(f)

log(f"\nğŸš€ íŒêµë„ì„œê´€ ì²­êµ¬ê¸°í˜¸ í¬ë¡¤ë§ ì‹œì‘... (ì´ {len(books)}ê¶Œ)")

BASE_URL = "https://www.snlib.go.kr/pg/plusSearchResultList.do"

results = []
success_count = 0
fail_count = 0

for i, book in enumerate(books, 1):
    year = book['year']
    korean_title = book.get('korean_title')
    original_title = book['original_title']
    author = book.get('author', '')
    illustrator = book.get('illustrator', '')
    
    # ê²€ìƒ‰ ìš°ì„ ìˆœìœ„: í•œê¸€ ì œëª© > ì›ì œ
    search_title = korean_title if korean_title else original_title
    
    # ì €ìëª… ì¶”ì¶œ (ì„±ë§Œ)
    author_key = author.split()[0] if author else ''
    illustrator_key = illustrator.split()[0] if illustrator else ''
    
    # ì •ê·œí™”ëœ íƒ€ê²Ÿ ì •ë³´
    norm_target_title = normalize(search_title)
    norm_target_author = normalize(author_key)
    norm_target_illustrator = normalize(illustrator_key)
    
    log(f"[{i}/{len(books)}] {year}ë…„ - {search_title}")
    
    current_result = {**book, 'pangyo_callno': None, 'crawl_status': 'fail'}
    
    try:
        params = {
            'searchKeyword': search_title,
            'searchType': 'SIMPLE',
            'searchCategory': 'BOOK',
            'searchLibraryArr': 'MP',
            'searchKey': 'ALL',
            'topSearchType': 'BOOK'
        }
        
        response = requests.get(BASE_URL, params=params, timeout=30)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            result_items = soup.select('ul.resultList > li')
            
            found_callno = None
            match_type = None
            
            if result_items:
                for idx, item in enumerate(result_items):
                    title_elem = item.select_one('.tit a')
                    result_title = title_elem.get_text(strip=True) if title_elem else ""
                    item_text = item.get_text(strip=True)
                    
                    norm_result_title = normalize(result_title)
                    norm_item_text = normalize(item_text)
                    
                    is_title_match = (norm_target_title in norm_result_title) or (norm_result_title in norm_target_title)
                    is_author_match = norm_target_author in norm_item_text if norm_target_author else False
                    is_illustrator_match = norm_target_illustrator in norm_item_text if norm_target_illustrator else False
                    
                    callno = None
                    dd_elements = item.select('dl dd')
                    for dd in dd_elements:
                        text = dd.get_text(strip=True)
                        if 'ì²­êµ¬ê¸°í˜¸' in text:
                            parts = text.split('ì²­êµ¬ê¸°í˜¸')
                            if len(parts) > 1:
                                candidate = parts[-1].replace(':', '').strip()
                                callno = candidate.split('ìœ„ì¹˜ì¶œë ¥')[0].strip()
                                break
                    
                    if not callno:
                        continue
                    
                    # Strict Match: ì œëª© + (ì €ì or ê·¸ë¦¼ì‘ê°€)
                    if is_title_match and (is_author_match or is_illustrator_match):
                        found_callno = callno
                        match_type = "Strict Match"
                        break
                    
                    # Fallback: ì œëª©ë§Œ ì¼ì¹˜
                    if not found_callno and is_title_match:
                        found_callno = callno
                        match_type = "Title Only"
                
                if found_callno:
                    current_result['pangyo_callno'] = found_callno
                    current_result['crawl_status'] = 'success'
                    current_result['match_type'] = match_type
                    success_count += 1
                    log(f"  âœ… {match_type}: {found_callno}")
                else:
                    current_result['crawl_status'] = 'mismatch'
                    fail_count += 1
                    log(f"  âš ï¸ ë§¤ì¹­ ì‹¤íŒ¨")
            else:
                current_result['crawl_status'] = 'not_found'
                fail_count += 1
                log(f"  âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
        else:
            current_result['crawl_status'] = 'http_error'
            current_result['error'] = str(response.status_code)
            fail_count += 1
            log(f"  âŒ HTTP Error: {response.status_code}")
        
        time.sleep(1.0)
        
    except Exception as e:
        current_result['crawl_status'] = 'error'
        current_result['error'] = str(e)
        fail_count += 1
        log(f"  âŒ ì—ëŸ¬: {e}")
        time.sleep(2)
    
    results.append(current_result)

log("")
log("="*50)
log(f"âœ… ì„±ê³µ: {success_count}ê¶Œ")
log(f"âŒ ì‹¤íŒ¨: {fail_count}ê¶Œ")
log("="*50)

# ê²°ê³¼ ì €ì¥
with open('caldecott_final.json', 'w', encoding='utf-8') as f:
    json.dump(results, f, ensure_ascii=False, indent=2)

log("\nâœ… caldecott_final.json íŒŒì¼ë¡œ ì €ì¥ ì™„ë£Œ!")

log_file.close()

# -*- coding: utf-8 -*-
import sys
import io
import os
import json
import time
import requests
import re
from bs4 import BeautifulSoup

# ì½˜ì†” ì¶œë ¥ ì¸ì½”ë”© ì„¤ì •
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# Helper for logging
log_file = open('crawling_progress.log', 'a', encoding='utf-8') # Append mode
def log(msg):
    print(msg)
    log_file.write(msg + '\n')
    log_file.flush()

# ì •ê·œí™” í•¨ìˆ˜: ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì œê±°
def normalize(text):
    if not text:
        return ""
    # ê´„í˜¸ì™€ ê·¸ ì•ˆì˜ ë‚´ìš© ì œê±° (ì˜ˆ: (ê°œì •íŒ), [ë„ì„œ] ë“±)
    text = re.sub(r'\(.*?\)|\[.*?\]', '', text)
    # íŠ¹ìˆ˜ë¬¸ì ì œê±°, ê³µë°± ì œê±°, ì†Œë¬¸ì ë³€í™˜
    text = re.sub(r'[\s\W_]+', '', text).lower()
    return text

# JSON íŒŒì¼ ì½ê¸°
with open('winter_books_clean.json', 'r', encoding='utf-8') as f:
    books = json.load(f)

log(f"\nğŸš€ í¬ë¡¤ë§ ì¬ì‹œì‘... (ì´ {len(books)}ê¶Œ)")

BASE_URL = "https://www.snlib.go.kr/pg/plusSearchResultList.do"

# Load existing progress
results = []
done_titles = set()

if os.path.exists('crawling_results.jsonl'):
    with open('crawling_results.jsonl', 'r', encoding='utf-8') as f:
        for line in f:
            try:
                data = json.loads(line)
                results.append(data)
                done_titles.add(data['title'])
            except: pass
    log(f"ğŸ“‹ ì´ì „ ê²°ê³¼ {len(results)}ê±´ ë¡œë“œ ì™„ë£Œ")

success_count = len([r for r in results if r.get('status') == 'success'])
fail_count = len([r for r in results if r.get('status') != 'success'])

for i, book in enumerate(books, 1):
    target_title = book['ì„œëª…']
    if target_title in done_titles:
        # log(f"[{i}/{len(books)}] {target_title} (Skip: ì´ë¯¸ ì™„ë£Œ)")
        continue

    target_author = book['ì €ì']
    target_publisher = book.get('ë°œí–‰ì', '')
    
    # ì €ì ì´ë¦„ ì²« ì–´ì ˆ ì¶”ì¶œ
    target_author_key = target_author.split()[0] if target_author else ""
    
    # ì •ê·œí™”ëœ íƒ€ê²Ÿ ì •ë³´
    norm_target_title = normalize(target_title)
    norm_target_author = normalize(target_author_key)
    norm_target_publisher = normalize(target_publisher)
    
    log(f"[{i}/{len(books)}] {target_title} / {target_author_key} (ì§„í–‰ì¤‘)")
    
    current_result = {'title': target_title, 'status': 'fail'} # Default

    try:
        params = {
            'searchKeyword': target_title,
            'searchType': 'SIMPLE',
            'searchCategory': 'BOOK',
            'searchLibraryArr': 'MP',
            'searchKey': 'ALL',
            'topSearchType': 'BOOK'
        }
        
        response = requests.get(BASE_URL, params=params, timeout=30)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            result_items = soup.select('ul.resultList > li')
            
            found_callno = None
            match_type = None
            
            if result_items:
                for idx, item in enumerate(result_items):
                    title_elem = item.select_one('.tit a')
                    result_title = title_elem.get_text(strip=True) if title_elem else ""
                    item_text = item.get_text(strip=True)
                    
                    norm_result_title = normalize(result_title)
                    norm_item_text = normalize(item_text)
                    
                    is_title_match = (norm_target_title in norm_result_title) or (norm_result_title in norm_target_title)
                    is_author_match = norm_target_author in norm_item_text
                    is_publisher_match = norm_target_publisher in norm_item_text if norm_target_publisher else False
                    
                    callno = None
                    dd_elements = item.select('dl dd')
                    for dd in dd_elements:
                        text = dd.get_text(strip=True)
                        if 'ì²­êµ¬ê¸°í˜¸' in text:
                            # "ì €ì: ... ì²­êµ¬ê¸°í˜¸: 123" í˜•íƒœì¼ ê²½ìš° "123"ë§Œ ì¶”ì¶œ
                            parts = text.split('ì²­êµ¬ê¸°í˜¸')
                            if len(parts) > 1:
                                candidate = parts[-1].replace(':', '').strip()
                                callno = candidate.split('ìœ„ì¹˜ì¶œë ¥')[0].strip()
                                break
                    
                    if not callno: continue

                    if is_title_match and is_author_match and is_publisher_match:
                        found_callno = callno
                        match_type = "Strict Match"
                        break 
                    
                    if not found_callno and is_title_match and is_author_match:
                        found_callno = callno
                        match_type = "Fallback Match"
                
                if found_callno:
                    current_result = {
                        'title': target_title,
                        'callno': found_callno,
                        'match_type': match_type,
                        'status': 'success'
                    }
                    success_count += 1
                    log(f"  âœ… {match_type}: {found_callno}")
                else:
                    current_result['status'] = 'mismatch'
                    fail_count += 1
                    log(f"  âš ï¸ ë§¤ì¹­ ì‹¤íŒ¨")
            else:
                current_result['status'] = 'not_found'
                fail_count += 1
                log(f"  âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
        else:
            current_result['status'] = 'http_error'
            current_result['error'] = str(response.status_code)
            fail_count += 1
            log(f"  âŒ HTTP Error: {response.status_code}")

        time.sleep(1.0) 

    except Exception as e:
        current_result['status'] = 'error'
        current_result['error'] = str(e)
        fail_count += 1
        log(f"  âŒ ì—ëŸ¬: {e}")
        time.sleep(2)
    
    # Save incrementally
    results.append(current_result)
    with open('crawling_results.jsonl', 'a', encoding='utf-8') as f:
        json.dump(current_result, f, ensure_ascii=False)
        f.write('\n')

# Final Report & SQL Generation
log("")
log("="*50)
log(f"âœ… ì„±ê³µ: {success_count}ê¶Œ")
log(f"âŒ ì‹¤íŒ¨: {fail_count}ê¶Œ")
log("="*50)

if success_count > 0:
    sql_lines = []
    sql_lines.append("-- ê²¨ìš¸ë°©í•™ ë„ì„œ ì²­êµ¬ê¸°í˜¸ ì •ë°€ ì—…ë°ì´íŠ¸ (Strict/Fallback Matching)")
    sql_lines.append(f"-- ì„±ê³µ: {success_count}/{len(books)}")
    sql_lines.append("")
    
    # Re-read results to ensure completeness
    # (Optional, but using memory 'results' is safer here to include previous runs)
    
    for res in results:
        if res.get('status') == 'success':
            title_esc = res['title'].replace("'", "''")
            callno_esc = res['callno'].replace("'", "''")
            sql = f"UPDATE childbook_items SET pangyo_callno = '{callno_esc}' WHERE title = '{title_esc}' AND curation_tag = 'ê²¨ìš¸ë°©í•™2026';"
            sql_lines.append(f"-- {res.get('match_type', 'Unknown')}")
            sql_lines.append(sql)
            
    with open('update_winter_callno_v2.sql', 'w', encoding='utf-8') as f:
        f.write('\n'.join(sql_lines))
    log("âœ… update_winter_callno_v2.sql ìƒì„± ì™„ë£Œ")
